[{
    "task_name":"Discrimination task (n out of K and n < K)",
    "task_description": "This is a classification problem. You should evaluate a pool of candidate items to distinguish and select a specific subset of options that the user is most likely to interact with, effectively separating the target 'top N' choices from the rest based on specific trade-offs (e.g., 'User prefers Item A over B and C').",
    "action_name": "discrimination/selection",
    "decision_graph_org": "Decision Graph (Causal):\n1. **User–Item Preference Matching**:\nFor each candidate book, estimate how strongly its attributes (e.g., genre/theme/author) match the user’s inferred preferences; this creates the initial evidence for whether an item should be in the selected subset.\n2. **Relative Preference Trade-off Comparison**:\nCompare candidates against each other to determine which items best satisfy the user’s higher-priority preferences (e.g., “A preferred over B and C”); changing this trade-off ordering changes which items enter the top-N set.\n3. **Selectiveness / Acceptance Thresholding**:\nDecide which items are “good enough” to be selected by applying a threshold reflecting how selective the user is; removing or shifting this threshold changes the size/composition of the selected subset and directly affects classification outcomes.\n4. **Subset Construction Under Cardinality Constraint**:\nConstruct the final selection set of size (N) by choosing the highest-confidence items after thresholding; this step is causal because different selection policies (e.g., strict top-N vs threshold-then-fill) can change membership of the selected set.\n5. **Boundary Tie-Break Resolution**:\nIf multiple items lie near the selection boundary (competing for the last slot(s)), apply deterministic tie-break rules (e.g., strongest match to primary preference, secondary preference priority) to decide inclusion; removing this step makes outputs unstable and can change which items are selected.",
    "decision_graph_new": "Decision Graph (Causal):\n1. Preference-Genre Compatibility: Evaluate if the item's genre aligns with the user's preferred genres. If compatible, the item is more likely to be selected.\n2. Preference-Content Relevance: Assess the relevance of the item's content to the user's interests. If highly relevant, the item is favored for selection.\n3. Item Popularity: Consider the popularity of the item among similar users. If popular, it increases the likelihood of selection.\n4. User Trade-offs: Compare the selected items against each other based on user preferences. If an item is deemed superior to others, it is selected over them.\n\nCausal Control Points:\n- Preference-Genre Compatibility: 1 | genre\n- Preference-Content Relevance: 1 | content relevance\n- Item Popularity: 1 | popularity\n- User Trade-offs: 2 | item comparison, user preferences",
    "decision_graph_notbad": "Decision Graph (Causal):\n\n1. Primary Preference Scoring:\nScore each candidate using the user’s strongest preference axis (e.g., core product type or intent). This determines which items can compete for rank #1.\n\n2. Near-Miss Rejection:\nDemote candidates that partially match preferences but miss key criteria. This prevents plausible but incorrect items from overtaking the true top choice.\n\n3. Priority Arbitration:\nWhen candidates match different preferences, apply explicit priority rules to decide which signal dominates. This resolves trade-offs that affect rank #1.\n\n4. Top-1 Tie-Break:\nIf multiple candidates remain close, apply a deterministic rule to select exactly one item for position #1.\n\n5. Selectiveness Calibration:\nControl how sharply the user separates the best item from the rest (strict vs tolerant), affecting how decisively the top item is promoted.",
    "decision_graph": "1. Primary Preferences and Tastes \n2. Priority Arbitration: When candidates match different preferences, apply explicit priority rules to decide which signal dominates. This resolves trade-offs. \n3. Tie-Break:\nIf multiple candidates remain close, apply a deterministic rule to select exactly items. \n4. Selectiveness Calibration:\nControl how sharply the user separates the best item from the rest (strict vs tolerant), affecting how decisively the top item is promoted."
}, {
    "task_name":"Rating prediction",
    "task_description": "Assign a rating score (1-5) to the provided items that the user has already interacted with based on the user's profile. The rating represents how much the user would like this item. It focuses on the intensity of satisfaction. Produce ratings that are consistent, preference-driven, and similar to how a real user would rate items. ",
    "action_name": "rating",
    "decision_graph_org": "Decision Graph (Causal):\nUser–Item Preference Alignment Estimation:\nFor each book, assess how strongly its attributes (e.g., genre and thematic content) align with the user’s inferred reading preferences; this establishes the baseline level of satisfaction and directly influences the rating magnitude.\nRelative Satisfaction Comparison Across Items:\nCompare the preference alignment of all interacted books to determine their relative enjoyment levels; altering this comparison would change which items receive higher or lower ratings.\nRating Scale Calibration and Boundary Mapping:\nMap relative satisfaction levels onto the user’s internal 1–5 rating scale, determining how differences in preference strength translate into discrete rating values (e.g., deciding whether an item merits a 4 versus a 5).\nConsistency Adjustment Across Similar Items:\nEnsure items with highly similar preference alignment receive consistent ratings, preventing unjustified variation; removing this step would lead to unstable or unrealistic rating behavior.\nCausal Control Points:\nStrength of genre and thematic preferences (drives preference alignment)\nRelative preference intensity between items (drives comparative satisfaction)\nUser-specific rating scale usage and strictness (drives calibration and boundary decisions \nTolerance for rating variation among similar items (drives consistency adjustment)",
    "decision_graph": "Decision Graph (Causal):\n1. User Preference Assessment: Determine the user's overall preferences based on their past ratings and interactions, which influences the predicted rating for new items.\n2. Item Feature Evaluation: Analyze the features of the items (e.g., book genre, author.) to assess compatibility with the user's preferences, affecting the final rating.\n3. MapFitToRating: convert the final preference fit into a 1–5 rating level.\n4. ApplyRatingStyle: adjust to match the user’s rating habits (lenient/harsh, uses extremes, scale calibration). \n5. TieBreakWithConsensus: if uncertain, slightly nudge using popularity/average rating without overriding strong preferences. \n\nCausalControlPoints: \n- InferUserPreferences: FavouriteGenres, PreferredThemes, PreferredTone, PreferenceStrength.\n- InterpretItemExperience: 0 user attributes. \n- JudgePreferenceFit: GenrePreference, ThemePreference, TonePreference, LikeStrength. \n- ApplyAvoidanceChecks: StrongDislikes, ContentSensitivities. \n- MapFitToRating: SatisfactionIntensity, FiveStarThreshold, BoredomTolerance. \n- ApplyRatingStyle: RatingLeniency, RatingExtremeness, TypicalVariance. \n- TieBreakWithConsensus: ConsensusSensitivity."
}, {
    "task_name":"Items ranking task",
    "task_description": "Analyse the candidates to identify ONE specific item with the highest probability of user interaction and strictly place it at the very top of the list, relegating less certain options to lower ranks. Only one item is the ground truth which user actually interacted with. Your goal is to find the Ground Truth item and place it at **Rank #1**",
    "action_name": "ranking",
    "decision_graph_causalcp": "Causal Control Points:\nPrimary genre preference(drive preference matching)\nSubgenre interest\nRelative preference strength across genres (drive dominance comparison)\nImplicit avoidance signals inferred from interaction history (drive rejection of near-miss items)\nTie-breaking tendencies under uncertainty (drive final rank-1 resolution)",
    "decision_graph_ml":"Decision Graph (Causal):\n1. **Primary Preference Assessment**: Evaluate user Primary preferences(genres, themes, or authors) based on historical interaction data to identify items with the highest likelihood of engagement. This affects the outcome by prioritising items that align with user interests.\n2. **Item Popularity Evaluation**: Analyse the popularity metrics of items to determine their general appeal. This affects the outcome by ensuring that widely appreciated items are considered for higher ranking.\n3. **TOP TWO Genre Relevance Check**: Compare the genres of items with the user's TOP TWO preferred genres. This affects the outcome by ensuring that items matching user genre preferences are prioritised.\n4. **Final Ranking Adjustment**: Adjust the final ranking to ensure the top item is the best match based on the previous evaluations. This affects the outcome by guaranteeing that the most suitable item is placed in position #1.\n\nCausal Control Points:\n- User historical interaction data (e.g., previously read books, ratings and reviews given)\n- Primary genre preferences (e.g., preferred genres based on past behaviour)\n- Item popularity analysis\n",
    "decision_graph": "Decision Graph (Causal):\n\n1. Primary Preference Scoring:\nScore each candidate using the user’s strongest preference axis (e.g., core book genres or intent). This determines which items can compete for rank #1.\n\n2. Near-Miss Rejection:\nDemote candidates that partially match preferences but miss key criteria. This prevents plausible but incorrect items from overtaking the true top choice.\n\n3. Priority Arbitration:\nWhen candidates match different preferences, apply explicit priority rules to decide which signal dominates. This resolves trade-offs that affect rank #1.\n\n4. Top-1 Tie-Break:\nIf multiple candidates remain close, apply a deterministic rule to select exactly one item for position #1.\n\n5. Selectiveness Calibration:\nControl how sharply the user separates the best item from the rest (strict vs tolerant), affecting how decisively the top item is promoted.",
    "decision_graph_5.2dg":"User–Item Preference Matching:\nFor each candidate book, estimate how well its content attributes (e.g., genre, themes) align with the user’s inferred reading preferences; this directly determines which items are eligible to compete for the top rank.\n\nRelative Preference Dominance Comparison:\nCompare candidate books against each other to determine which item best satisfies the user’s strongest preferences; changing this comparison alters which item is placed at rank #1.\n\nImplicit Rejection of Near-Miss Candidates:\nDeprioritize items whose attributes weakly match or partially conflict with the user’s inferred preferences, even if they have high popularity or ratings; removing this step would allow non-preferred but popular items to overtake the true positive.\n\nTie-Break Resolution Under Preference Ambiguity:\nWhen multiple items are similarly preferred, apply secondary user tendencies (e.g., tolerance for unfamiliar genres, sensitivity to thematic alignment) to resolve which single item is ranked first; this step is decisive for top-1 placement.\n\nCausal Control Points:\nPrimary genre and theme preferences (drive preference matching)\nRelative preference strength across genres (drive dominance comparison)\nImplicit avoidance signals inferred from interaction history (drive rejection of near-miss items)\nTie-breaking tendencies under uncertainty (drive final rank-1 resolution)",
    "decision_graph_goodforgpt5.1": "Decision Graph (Causal): \n1. Candidate Evaluation: Determine for each candidate item its estimated probability of user interaction; this creates the basis for comparing items and directly affects which item can be ranked first. \n2. Top-Probability Selection: Identify the single item with the highest estimated interaction probability; this selection causally determines the ground-truth candidate to place at Rank #1. \n3. Priority Assignment: Place the selected top-probability item at Rank #1 and relegate all other items below it; this step causally establishes the final ranking order. \n\nCausal Control Points: \n- User interaction history \n- User content preferences or embeddings \n- Contextual features (time, session signals) \n- Item attributes or embeddings"
}]